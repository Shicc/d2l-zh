{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data import VOCDetection\n",
    "\n",
    "train_dataset = VOCDetection(root = '../../../data/VOCdevkit',splits=[(2012, 'train')]) # 这是在VOC2012\\ImageSets\\Main里面找对应名字的txt\n",
    "val_dataset = VOCDetection(root = '../../../data/VOCdevkit', splits=[(2012, 'val')])\n",
    "\n",
    "print('Training images:', len(train_dataset))\n",
    "print('Validation images:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv import model_zoo\n",
    "# pretrained_base = True 会加载模型\n",
    "net = model_zoo.get_model('ssd_300_vgg16_atrous_voc', pretrained_base=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像增广对于ssd网络来说很重要，下面就进行增广\n",
    "from gluoncv.data.transforms import presets\n",
    "from gluoncv import utils\n",
    "from mxnet import nd\n",
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet import autograd\n",
    "import mxnet as mx\n",
    "\n",
    "x = nd.zeros(shape=(1,3,512,512))\n",
    "ctx = mx.gpu() # 单个gpu时\n",
    "net.initialize(init=init.Xavier(), ctx=ctx)\n",
    "\n",
    "with autograd.train_mode():\n",
    "    cls_preds, box_preds, anchors = net(x)\n",
    "\n",
    "width, height = 512, 512  # 假设512*512大小\n",
    "train_transform = presets.ssd.SSDDefaultTrainTransform(width, height,anchors)\n",
    "val_transform = presets.ssd.SSDDefaultValTransform(width, height, anchors)\n",
    "\n",
    "batch_size = 64 # 试运行，弄小点方便看 .训练时调大点。比如128\n",
    "# you can make it larger(if your CPU has more cores) to accelerate data loading\n",
    "num_workers = 2 \n",
    "\n",
    "# behavior of batchify_fn: stack images, and pad labels\n",
    "batchify_fn = Tuple(Stack(), Stack(), Stack()) # train_transform 有三个参数，这里也需要3个\n",
    "train_loader = DataLoader(\n",
    "    train_dataset.transform(train_transform),\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    batchify_fn=batchify_fn,\n",
    "    last_batch='rollover',\n",
    "    num_workers=num_workers) # 是否多线程\n",
    "# val_loader = DataLoader(\n",
    "#     val_dataset.transform(val_transform),\n",
    "#     batch_size,\n",
    "#     shuffle=False,\n",
    "#     batchify_fn=batchify_fn,\n",
    "#     last_batch='keep',\n",
    "#     num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.loss import SSDMultiBoxLoss\n",
    "from mxnet import gluon\n",
    "\n",
    "mbox_loss = SSDMultiBoxLoss()\n",
    "ce_metric = mx.metric.Loss('CrossEntropy')\n",
    "smoothl1_metric = mx.metric.Loss('SmoothL1')\n",
    "trainer = gluon.Trainer(\n",
    "    net.collect_params(), 'sgd',\n",
    "    {'learning_rate': 0.001, 'wd': 0.0005, 'momentum': 0.9})\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()\n",
    "    train_loader.reset()  # 从头读取数据\n",
    "    for ib, batch in enumerate(train_loader):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
    "        with autograd.record():\n",
    "           cls_preds = []\n",
    "           box_preds = []\n",
    "           for x in data:\n",
    "               cls_pred, box_pred, _ = net(x)\n",
    "               cls_preds.append(cls_pred)\n",
    "               box_preds.append(box_pred)\n",
    "           sum_loss, cls_loss, box_loss = mbox_loss(\n",
    "                    cls_preds, box_preds, cls_targets, box_targets)\n",
    "           autograd.backward(sum_loss)\n",
    "           # since we have already normalized the loss, we don't want to normalize\n",
    "           # by batch-size anymore\n",
    "        trainer.step(1)\n",
    "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
    "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
    "        name1, loss1 = ce_metric.get()\n",
    "        name2, loss2 = smoothl1_metric.get()\n",
    "#     print('epoch %2d, class err %.2e, bbox mae %.2e, time %.1f sec' % (\n",
    "#             epoch + 1, 1 - loss1 / n, loss2 / m, time.time() - start))\n",
    "     print('epoch:', epoch, 'class loss:', loss1, 'bbox loss:', loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "img = image.imread('d2l-zh/img/pikachu.jpg')\n",
    "feature = image.imresize(img, 256, 256).astype('float32')\n",
    "X = feature.transpose((2, 0, 1)).expand_dims(axis=0)\n",
    "\n",
    "def predict(X):\n",
    "    anchors, cls_preds, bbox_preds = net(X.as_in_context(ctx))\n",
    "    cls_probs = cls_preds.softmax().transpose((0, 2, 1))\n",
    "    output = contrib.nd.MultiBoxDetection(cls_probs, bbox_preds, anchors)\n",
    "    idx = [i for i, row in enumerate(output[0]) if row[0].asscalar() != -1]\n",
    "    return output[0, idx]\n",
    "\n",
    "output = predict(X)\n",
    "\n",
    "# 显示\n",
    "d2l.set_figsize((5, 5))\n",
    "\n",
    "def display(img, output, threshold):\n",
    "    fig = d2l.plt.imshow(img.asnumpy())\n",
    "    for row in output:\n",
    "        score = row[1].asscalar()\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        h, w = img.shape[0:2]\n",
    "        bbox = [row[2:6] * nd.array((w, h, w, h), ctx=row.context)]\n",
    "        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n",
    "\n",
    "display(img, output, threshold=0.3) # 选出置信度不低于0.3的边界框"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
